{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process of building Deep Neural Network:\n",
    "- Import Dataset\n",
    "- Load Dataset into **Train and Test**.\n",
    "- **Normalize** Train and Test datasets.\n",
    "- **Construct** the Model(with layers).\n",
    "- **Configure** the Model for **Training**.\n",
    "- **Train the Model** by Fitting Training data with appropriate labels.\n",
    "- **Evaluate loss and accuracy** on **validation dataset**.\n",
    "- Save the model(if satisfied with accuracy)\n",
    "- Re-instatiate the saved model.\n",
    "- **Predictions on Test/Real-life dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to build Deep Learning Model using 2 approaches and compare the accuracy of both the models with respect to each other:\n",
    "- Approach:1--> Using traditional approach i.e. not using Convolution and Pooling layers\n",
    "- Approach:2--> Using Convolution and Pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dataset:\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "#Load the dataset:\n",
    "(train_images,train_labels),(test_images,test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHAPE-TRAIN DATASET]:: (60000, 28, 28)\n",
      "[SHAPE-TEST DATASET]:: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#Check Train and Test datasets:\n",
    "#View shapes:\n",
    "print(\"[SHAPE-TRAIN DATASET]::\",train_images.shape)\n",
    "print(\"[SHAPE-TEST DATASET]::\",test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.cm.get_cmap(name=None, lut=None)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.cm.get_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#View some of the dataset:\n",
    "'''\n",
    "# cmap='gray'::Image display in \"Gray-Scale\"\n",
    "# cmap='gray_r'::Image display in \"Inverse Gray-Scale\"\n",
    "    # Gray-Scale--> Shades \n",
    "    # Inverse Gray-Scale-->\n",
    "'''\n",
    "plt.imshow(train_images[55000],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#While displaying the image as binary--Thresholding operation perform.\n",
    "plt.imshow(train_images[55000],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom PIL import Image\\n\\nfname = \\'image.png\\'\\nimage = Image.open(fname).convert(\"L\")\\narr = np.asarray(image)\\nplt.imshow(arr, cmap=\\'gray\\', vmin=0, vmax=255)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The following code will load an image from a file image.png and will display it as grayscale:\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "fname = 'image.png'\n",
    "image = Image.open(fname).convert(\"L\")\n",
    "arr = np.asarray(image)\n",
    "plt.imshow(arr, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Phase\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model_traditional = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "                                               tf.keras.layers.Dense(128,activation=tf.nn.relu),\n",
    "                                               tf.keras.layers.Dense(256,activation=tf.nn.relu),\n",
    "                                               tf.keras.layers.Dense(10,activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Model Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 136,074\n",
      "Trainable params: 136,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_traditional.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Model for Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_traditional.compile(optimizer='adam',loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                         metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Training data with Training labels and specify number of epochs for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 8s 141us/sample - loss: 0.4763 - acc: 0.8298\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3615 - acc: 0.8674\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.3250 - acc: 0.8795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2276dc7ca48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_traditional.fit(train_images,train_labels,epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model's accuracy on Validation Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 77us/sample - loss: 0.3849 - acc: 0.8651\n",
      "[LOSS-VALIDATION]:: 0.38493964943885806\n",
      "[ACCURACY-VALIDATION]:: 0.8651\n"
     ]
    }
   ],
   "source": [
    "val_loss,val_acc = model_traditional.evaluate(test_images,test_labels)\n",
    "print(\"[LOSS-VALIDATION]::\",val_loss)\n",
    "print(\"[ACCURACY-VALIDATION]::\",val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is about **87% on Training Dataset** and the same(not usually happen) **87% on validation dataset**. Not Bad..! But we need to think ahead and rethink how do we make this accuracy even better? One way to use **\"Convolutions\"**. In short, Convolution operation **narrow down** the contents of the image to focous on specific and distinct details.\n",
    "\n",
    "Feature Extraction::This is perfect for Computer Vision,because it's features that can highlights and distingquish **one object from another** and then the amount of information needed is much less for training,because we'll just train on Highlighted features. And this is the concept behind **Convolutional Neural Networks**.\n",
    "\n",
    "The Convolution layers are before Dense layers with the purpose of information going to the dense layers is more focussed, and possibly more accurate(fetaures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "#Import tensorflow module:\n",
    "import tensorflow as tf\n",
    "#Print its vesion:\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Fashion MNIST dataset and load it into train and test dataset:\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images,train_labels),(test_images,test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHAPE->TRAINING DATASET]:: (60000, 28, 28)\n",
      "[SHAPE->TESTING DATASET]:: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#Check the shape of training and testing dataset:\n",
    "print(\"[SHAPE->TRAINING DATASET]::\",train_images.shape)\n",
    "print(\"[SHAPE->TESTING DATASET]::\",test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape the dataset--> That's because the first convolution layer expects a single tensor containing everything, \n",
    "#so instead of (60,000 28x28)(color channel is not specified) items in a list, \n",
    "#we have a single 4D list that is (60,000x28x28x1)--Added color channel as 1-GrayScale image,3-Color image, \n",
    "#and the same for the test images and then normalize the datasets:\n",
    "train_images = train_images.reshape(60000,28,28,1)\n",
    "test_images = test_images.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHAPE->TRAINING DATASET]:: (60000, 28, 28, 1)\n",
      "[SHAPE->TESTING DATASET]:: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#Check shape again:\n",
    "print(\"[SHAPE->TRAINING DATASET]::\",train_images.shape)\n",
    "print(\"[SHAPE->TESTING DATASET]::\",test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the datasets as usual method:\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Convolution Layers, the Parameters are:\n",
    "- The number of Convolutions(feature maps) we want to generate. Purely arbitrary, but good to start with something in the order of 32\n",
    "- The size of Convolution Kernel. In our case,we took 3x3 grid.\n",
    "- The activation function. In our case,we used relu.\n",
    "- The shape of the input data.\n",
    "- Next, the Convolution layer follows with a **MaxPooling layer** which use to **compress the image**, but **maintaining the content of the features** that were highlighted or we can say extracted by the convlution layer.\n",
    "- We are going to add 2 Convolution Layers and MaxPooling layer follows by each Convolution Layer.\n",
    "- View Model summary to see the size and shape of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to create same 'Sequential Model' as previous but with additional 'Convolution' and 'Pooling' layers:\n",
    "#We are going to add layers in array format::\n",
    "# Model type:: Sequential\n",
    "# Type of layers:: 2--> Convolutions 2-->Pooling 1-->Flatten 2-->Dense 1-->Output\n",
    "model_new = tf.keras.models.Sequential([tf.keras.layers.Conv2D(32,(3,3),activation=tf.nn.relu,input_shape=(28,28,1)),\n",
    "                                       tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "                                       tf.keras.layers.Conv2D(64,(3,3),activation=tf.nn.relu),\n",
    "                                       tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "                                       tf.keras.layers.Flatten(),\n",
    "                                       tf.keras.layers.Dense(128,activation=tf.nn.relu),\n",
    "                                       tf.keras.layers.Dense(256,activation=tf.nn.relu),\n",
    "                                       tf.keras.layers.Dense(10,activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 259,338\n",
      "Trainable params: 259,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print Model Summary:\n",
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the model for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model and pass 3 parameters--:\n",
    "    # Optimizer \n",
    "    # loss\n",
    "    # metrics to measure \n",
    "model_new.compile(optimizer='adam',loss=tf.keras.losses.sparse_categorical_crossentropy,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model with Training data and note the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 73s 1ms/sample - loss: 0.4618 - acc: 0.83130s - loss: 0.4624 - acc: 0.8\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 72s 1ms/sample - loss: 0.2988 - acc: 0.8894\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 77s 1ms/sample - loss: 0.2497 - acc: 0.9067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2276a8dc0c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new.fit(train_images,train_labels,epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Conclusion**:\n",
    "    - The Training **accuracy is significatly increase** compared with our first model(without convolution layers).\n",
    "    - We can increase more epochs as loss is seems decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 396us/sample - loss: 0.2622 - acc: 0.9040\n",
      "[TEST LOSS]:: 0.2622016505479813\n",
      "[TEST ACCURACY]:: 0.904\n"
     ]
    }
   ],
   "source": [
    "#Let's evaluate model on Validation dataset and note test loss and test accuracy:\n",
    "test_loss,test_accuracy = model_new.evaluate(test_images,test_labels)\n",
    "print(\"[TEST LOSS]::\",test_loss)\n",
    "print(\"[TEST ACCURACY]::\",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Convolutions and Pooling operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
      " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
      " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import plotting library:\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,Y_train),(X_test,Y_test)  = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x227688b2248>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x227688b2788>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x227688b2808>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x227688ba0c8>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x227688ba408>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x227688ba908>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x227688bc148>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x227688bc708>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'conv2d/Relu:0' shape=(?, 26, 26, 32) dtype=float32>]\n",
      "[<tf.Tensor 'conv2d/Relu:0' shape=(?, 26, 26, 32) dtype=float32>, <tf.Tensor 'max_pooling2d/MaxPool:0' shape=(?, 13, 13, 32) dtype=float32>]\n",
      "[<tf.Tensor 'conv2d/Relu:0' shape=(?, 26, 26, 32) dtype=float32>, <tf.Tensor 'max_pooling2d/MaxPool:0' shape=(?, 13, 13, 32) dtype=float32>, <tf.Tensor 'conv2d_1/Relu:0' shape=(?, 11, 11, 64) dtype=float32>]\n",
      "[<tf.Tensor 'conv2d/Relu:0' shape=(?, 26, 26, 32) dtype=float32>, <tf.Tensor 'max_pooling2d/MaxPool:0' shape=(?, 13, 13, 32) dtype=float32>, <tf.Tensor 'conv2d_1/Relu:0' shape=(?, 11, 11, 64) dtype=float32>, <tf.Tensor 'max_pooling2d_1/MaxPool:0' shape=(?, 5, 5, 64) dtype=float32>]\n",
      "[<tf.Tensor 'conv2d/Relu:0' shape=(?, 26, 26, 32) dtype=float32>, <tf.Tensor 'max_pooling2d/MaxPool:0' shape=(?, 13, 13, 32) dtype=float32>, <tf.Tensor 'conv2d_1/Relu:0' shape=(?, 11, 11, 64) dtype=float32>, <tf.Tensor 'max_pooling2d_1/MaxPool:0' shape=(?, 5, 5, 64) dtype=float32>, <tf.Tensor 'flatten_1/Reshape:0' shape=(?, 1600) dtype=float32>]\n",
      "[<tf.Tensor 'conv2d/Relu:0' shape=(?, 26, 26, 32) dtype=float32>, <tf.Tensor 'max_pooling2d/MaxPool:0' shape=(?, 13, 13, 32) dtype=float32>, <tf.Tensor 'conv2d_1/Relu:0' shape=(?, 11, 11, 64) dtype=float32>, <tf.Tensor 'max_pooling2d_1/MaxPool:0' shape=(?, 5, 5, 64) dtype=float32>, <tf.Tensor 'flatten_1/Reshape:0' shape=(?, 1600) dtype=float32>, <tf.Tensor 'dense_3/Relu:0' shape=(?, 128) dtype=float32>]\n",
      "[<tf.Tensor 'conv2d/Relu:0' shape=(?, 26, 26, 32) dtype=float32>, <tf.Tensor 'max_pooling2d/MaxPool:0' shape=(?, 13, 13, 32) dtype=float32>, <tf.Tensor 'conv2d_1/Relu:0' shape=(?, 11, 11, 64) dtype=float32>, <tf.Tensor 'max_pooling2d_1/MaxPool:0' shape=(?, 5, 5, 64) dtype=float32>, <tf.Tensor 'flatten_1/Reshape:0' shape=(?, 1600) dtype=float32>, <tf.Tensor 'dense_3/Relu:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'dense_4/Relu:0' shape=(?, 256) dtype=float32>]\n",
      "[<tf.Tensor 'conv2d/Relu:0' shape=(?, 26, 26, 32) dtype=float32>, <tf.Tensor 'max_pooling2d/MaxPool:0' shape=(?, 13, 13, 32) dtype=float32>, <tf.Tensor 'conv2d_1/Relu:0' shape=(?, 11, 11, 64) dtype=float32>, <tf.Tensor 'max_pooling2d_1/MaxPool:0' shape=(?, 5, 5, 64) dtype=float32>, <tf.Tensor 'flatten_1/Reshape:0' shape=(?, 1600) dtype=float32>, <tf.Tensor 'dense_3/Relu:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'dense_4/Relu:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'dense_5/Softmax:0' shape=(?, 10) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = []\n",
    "for layer in model_new.layers:\n",
    "    layer_outputs.append(layer.output)\n",
    "    print(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axarr = plt.subplots(3,4,figsize=(10,5))\n",
    "FIRST_IMAGE=0\n",
    "SECOND_IMAGE=10\n",
    "THIRD_IMAGE=20\n",
    "CONVOLUTION_NUMBER = 1\n",
    "\n",
    "from tensorflow.keras import models\n",
    "\n",
    "layer_outputs = []\n",
    "for layer in model_new.layers:\n",
    "    layer_outputs.append(layer.output)\n",
    "    \n",
    "activation_model = tf.keras.models.Model(inputs = model_new.input, outputs = layer_outputs)\n",
    "\n",
    "for x in range(0,4):\n",
    "    f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1,28,28,1))[x]\n",
    "    axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER],cmap='inferno')\n",
    "    axarr[0,x].grid(False)\n",
    "    \n",
    "    f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1,28,28,1))[x]\n",
    "    axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER],cmap='inferno')\n",
    "    axarr[1,x].grid(False)\n",
    "    \n",
    "    f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1,28,28,1))[x]\n",
    "    axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER],cmap='inferno')\n",
    "    axarr[2,x].grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'IMAGE-AT INDEX 20')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure,axes = plt.subplots(1,3,figsize=(10,5))\n",
    "axes[0].imshow(X_test[FIRST_IMAGE],cmap='gray')\n",
    "axes[0].set_title(\"IMAGE-AT INDEX {}\".format(FIRST_IMAGE))\n",
    "axes[1].imshow(X_test[SECOND_IMAGE],cmap='gray')\n",
    "axes[1].set_title(\"IMAGE-AT INDEX {}\".format(SECOND_IMAGE))\n",
    "axes[2].imshow(X_test[THIRD_IMAGE],cmap='gray')\n",
    "axes[2].set_title(\"IMAGE-AT INDEX {}\".format(THIRD_IMAGE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **EXPERIMENTS-**\n",
    "    - Editing the Convolutions(the number of filters). Change the standard 32s to either 16 or 64. Measure its impact on accuracy and on training time.\n",
    "    - Removation of final Convolution layer. Measure its impact on accuracy and on training time?\n",
    "    - Effect of adding more Convolution layers? Measure its impact.\n",
    "    - **Removation all Convolutions without the the first one. Measure its impact.**\n",
    "    - Implementation of callback to check on the loss function and to stop training once it hit a certain amount of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT :: 1-->\n",
    "   - Objective:: \n",
    "       - Editing the **Number of Convolutions(number of filters)** and note its impact on accuracy and training time.\n",
    "   \n",
    "   _[NOTE:We are keeping rest of the parameters(**number of neurons in Dense layers,activation function in both Convolution and Dense layers**) same to measure extact impact of Change in Filter numbers on Accuracy and Training Time]_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-1--> Construct the Model::\n",
    "#Create an instance of class 'Sequential'(it basically tells that our Model consist stack of layers):\n",
    "model_exp_1 = tf.keras.models.Sequential()\n",
    "\n",
    "#Add layers to model:\n",
    "#Specifiy 8 filters of 3x3 at 1st layer-->\n",
    "model_exp_1.add(tf.keras.layers.Conv2D(8,(3,3),activation=tf.nn.relu,input_shape=(28,28,1)))\n",
    "model_exp_1.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Specifiy 16 filters of 3x3 at 2nd layer-->\n",
    "model_exp_1.add(tf.keras.layers.Conv2D(16,(3,3),activation=tf.nn.relu))\n",
    "model_exp_1.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Input Flatten layer for gievn to Dense Layer-->\n",
    "model_exp_1.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#Add 2 Dense layers-->\n",
    "model_exp_1.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
    "model_exp_1.add(tf.keras.layers.Dense(256,activation=tf.nn.relu))\n",
    "\n",
    "#Add Output layers-->\n",
    "model_exp_1.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               51328     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 88,170\n",
      "Trainable params: 88,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print Model summary:\n",
    "model_exp_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuare/Compile the model for training:\n",
    "model_exp_1.compile(optimizer='adam',\n",
    "                   loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 30s 508us/sample - loss: 0.5152 - acc: 0.8079 - lo\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 31s 510us/sample - loss: 0.3490 - acc: 0.8721\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 35s 576us/sample - loss: 0.3003 - acc: 0.8889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22779289608>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model with Training data and specify number of epochs:\n",
    "model_exp_1.fit(train_images,train_labels,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 219us/sample - loss: 0.3173 - acc: 0.8838\n",
      "[TEST LOSS]:: 0.3173277768373489\n",
      "[TEST ACCURACY]:: 0.8838\n"
     ]
    }
   ],
   "source": [
    "#Let's evaluate model on Validation dataset and note test loss and test accuracy:\n",
    "test_loss,test_accuracy = model_exp_1.evaluate(test_images,test_labels)\n",
    "print(\"[TEST LOSS]::\",test_loss)\n",
    "print(\"[TEST ACCURACY]::\",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results::**\n",
    "- Observations-->\n",
    "    - Total Training time required(in case of 32 filters in first Convolution layer and 64 filters in second Convolution layer):: 222 Seconds\n",
    "    - Accuracy:: 90%\n",
    "\n",
    "    - Total Training time required(in case of 16 filters in first Convolution layer and 32 filters in second Convolution layer):: 96 Seconds\n",
    "    - Accuracy:: 88%\n",
    "\n",
    "**Conclusion::**\n",
    "- By observing the above result we can say that if we reduce the number of filters in first and second convolution layers the training time gets decrease but accuracy is 88%.\n",
    "- And if we increase the number of filters in first and second convolution layers the training time gets increase and we receive the training and testing accuracy around 90% which is good to consider.\n",
    "\n",
    "**Sol::**\n",
    "- In the case of more number of filters in 1st and 2nd case we can use **GPU** support to reduce the training time or I can say to perform training faster. But we need to go with the first choice i.e.more number of filters in 1st and 2nd layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT :: 2-->\n",
    "Objective:: \n",
    "- Removation of final Convolution layer. \n",
    "- Measurement of its impact on accuracy and on training time.\n",
    "\n",
    "[NOTE:We are keeping rest of the parameters(number of neurons in Dense layers,activation function in both Convolution and Dense layers) same to measure extact impact of Change in Filter numbers on Accuracy and Training Time]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-1--> Construct the Model::\n",
    "#Create an instance of class 'Sequential'(it basically tells that our Model consist stack of layers):\n",
    "model_exp_2 = tf.keras.models.Sequential()\n",
    "\n",
    "#Add layers to model:\n",
    "#Specifiy 32 filters of 3x3 at 1st layer-->\n",
    "model_exp_2.add(tf.keras.layers.Conv2D(32,(3,3),activation=tf.nn.relu,input_shape=(28,28,1)))\n",
    "model_exp_2.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Specifiy 16 filters of 3x3 at 2nd layer-->\n",
    "#model_exp_1.add(tf.keras.layers.Conv2D(16,(3,3),activation=tf.nn.relu))\n",
    "#[NOTE::We are keeping the pooling layer as it is-->]\n",
    "model_exp_2.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Input Flatten layer for gievn to Dense Layer-->\n",
    "model_exp_2.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#Add 2 Dense layers-->\n",
    "model_exp_2.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
    "model_exp_2.add(tf.keras.layers.Dense(256,activation=tf.nn.relu))\n",
    "\n",
    "#Add Output layers-->\n",
    "model_exp_2.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 183,498\n",
      "Trainable params: 183,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print Model summary:\n",
    "model_exp_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuare/Compile the model for training:\n",
    "model_exp_2.compile(optimizer='adam',\n",
    "                   loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 37s 620us/sample - loss: 0.4640 - acc: 0.8277\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 36s 605us/sample - loss: 0.3116 - acc: 0.8834\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 36s 602us/sample - loss: 0.2741 - acc: 0.8978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x227782a4508>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model with Training data and specify number of epochs:\n",
    "model_exp_2.fit(train_images,train_labels,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 232us/sample - loss: 0.3068 - acc: 0.8814\n",
      "[TEST LOSS]:: 0.30683049824237824\n",
      "[TEST ACCURACY]:: 0.8814\n"
     ]
    }
   ],
   "source": [
    "#Let's evaluate model on Validation dataset and note test loss and test accuracy:\n",
    "test_loss,test_accuracy = model_exp_2.evaluate(test_images,test_labels)\n",
    "print(\"[TEST LOSS]::\",test_loss)\n",
    "print(\"[TEST ACCURACY]::\",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results::**\n",
    "- Observations-->\n",
    "- **Case-1::**\n",
    "    - Total Training time required(in case of **32 filters in first Convolution layer** and **64 filters in second Convolution layer**):: 222 Seconds\n",
    "    - Accuracy::\n",
    "        - Training::90%\n",
    "        - Testing::90%\n",
    "- **Case-2::**\n",
    "    - Total Training time required(in case of **16 filters in first Convolution layer** and **32 filters in second Convolution layer**):: 96 Seconds\n",
    "    - Accuracy::\n",
    "        - Training::88%\n",
    "        - Testing::88%\n",
    "- **Case-3::**    \n",
    "    - Total Training time required(by **droping second Convolution layer** but **keeping second Pooling layer** as it is)::109 Seconds \n",
    "    - Accuracy::\n",
    "        - Training::89%\n",
    "        - Testing::88%\n",
    "\n",
    "**Conclusion::**\n",
    "- By observing the above result we can say that if we drop the second covolution layer but keep 2nd Pooling layer as it is we get Training accuracy is 89%(which is slightly less in Case-1) in 109 Seconds.\n",
    "\n",
    "**Sol::**\n",
    "- To reduce this training time even further or I can say to perform training more faster we can use **GPU** support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT :: 3-->\n",
    "- **Objective::** \n",
    "    - Effect of adding more Convolution layers. \n",
    "    - Measure its impact on Accuracy and Training Time.\n",
    "\n",
    "_[NOTE:We are keeping rest of the parameters**(number of neurons in Dense layers,activation function in both Convolution and Dense layers)** same to measure extact impact of Change in Filter numbers on Accuracy and Training Time]_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-1--> Construct the Model::\n",
    "#Create an instance of class 'Sequential'(it basically tells that our Model consist stack of layers):\n",
    "model_exp_3 = tf.keras.models.Sequential()\n",
    "\n",
    "#Add layers to model:\n",
    "#Specifiy 16 filters of 3x3 at 1st Convolution layer-->\n",
    "model_exp_3.add(tf.keras.layers.Conv2D(16,(3,3),activation=tf.nn.relu,input_shape=(28,28,1)))\n",
    "model_exp_3.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Specifiy 32 filters of 3x3 at 2nd Convolution layer-->\n",
    "model_exp_3.add(tf.keras.layers.Conv2D(32,(3,3),activation=tf.nn.relu))\n",
    "model_exp_3.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Specifiy 64 filters of 3x3 at 3rd Convolution layer-->\n",
    "model_exp_3.add(tf.keras.layers.Conv2D(64,(3,3),activation=tf.nn.relu))\n",
    "model_exp_3.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#Input Flatten layer for gievn to Dense Layer-->\n",
    "model_exp_3.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#Add 2 Dense layers-->\n",
    "model_exp_3.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
    "model_exp_3.add(tf.keras.layers.Dense(256,activation=tf.nn.relu))\n",
    "\n",
    "#Add Output layers-->\n",
    "model_exp_3.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 67,210\n",
      "Trainable params: 67,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print Model summary:\n",
    "model_exp_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuare/Compile the model for training:\n",
    "model_exp_3.compile(optimizer='adam',\n",
    "                   loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 44s 731us/sample - loss: 0.6210 - acc: 0.7710\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 44s 731us/sample - loss: 0.4065 - acc: 0.8518\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 44s 729us/sample - loss: 0.3524 - acc: 0.8698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22779b35348>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model with Training data and specify number of epochs:\n",
    "model_exp_3.fit(train_images,train_labels,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 257us/sample - loss: 0.3903 - acc: 0.8529\n",
      "[TEST LOSS]:: 0.3902632870674133\n",
      "[TEST ACCURACY]:: 0.8529\n"
     ]
    }
   ],
   "source": [
    "#Let's evaluate model on Validation dataset and note test loss and test accuracy:\n",
    "test_loss,test_accuracy = model_exp_3.evaluate(test_images,test_labels)\n",
    "print(\"[TEST LOSS]::\",test_loss)\n",
    "print(\"[TEST ACCURACY]::\",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results::**\n",
    "- Observations-->\n",
    "- **Case-1::**\n",
    "    - Total Training time required(in case of **32 filters in first Convolution layer** and **64 filters in second Convolution layer**):: 222 Seconds\n",
    "    - Accuracy::\n",
    "        - Training::90%\n",
    "        - Testing::90%\n",
    "- **Case-2::**\n",
    "    - Total Training time required(in case of **16 filters in first Convolution layer** and **32 filters in second Convolution layer**):: 96 Seconds\n",
    "    - Accuracy::\n",
    "        - Training::88%\n",
    "        - Testing::88%\n",
    "- **Case-3::**    \n",
    "    - Total Training time required(by **droping second Convolution layer** but **keeping second Pooling layer** as it is)::109 Seconds \n",
    "    - Accuracy::\n",
    "        - Training::89%\n",
    "        - Testing::88%\n",
    "- **Case-4::**    \n",
    "    - Total Training time required(by **adding more convolution and pooling layer**)::132 Seconds \n",
    "    - Accuracy::\n",
    "        - Training::86%\n",
    "        - Testing::85%\n",
    "\n",
    "**Conclusion::**\n",
    "- By observing the above result we can say that if we add more convolution as well as pooling layer in our model we get Training accuracy is 86%(which is less in all 3 previous cases) in 132 Seconds.\n",
    "\n",
    "**Sol::**\n",
    "- Need to find sweet point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT :: 4-->\n",
    "Objective:: \n",
    "   - Removation of all Convolution layers except 1st one.\n",
    "   - Measure effect on Accuracy and Training time.\n",
    "\n",
    "[NOTE:We are keeping rest of the parameters**(number of neurons in Dense layers,activation function in both Convolution and Dense layers)** same to measure extact impact of Change in Filter numbers on Accuracy and Training Time]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT :: 5-->\n",
    "Objective:: \n",
    "   - Implementation of CallBack function.\n",
    "   - Going with Case-1 approach for building the model.\n",
    "\n",
    "[NOTE:We are keeping rest of the parameters(number of neurons in Dense layers,activation function in both Convolution and Dense layers) same to measure extact impact of Change in Filter numbers on Accuracy and Training Time]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement CallBack:\n",
    "#Define a class named myCallBack(which is going to derive from superclass \"CallBack\"):\n",
    "class myCallBack(tf.keras.callbacks.Callback):\n",
    "    #Define a function for monitoring accuracy after each epoch end:\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        if logs.get('acc') > 0.91:\n",
    "            print(\"\\n[MESSAGE]::Reached 91% accuracy so stopped further training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "#Create an instance of myCallBack class:\n",
    "call_back = myCallBack()\n",
    "\n",
    "#Construct the Model:\n",
    "model_exp_5 = tf.keras.models.Sequential([tf.keras.layers.Conv2D(32,(3,3),activation=tf.nn.relu,input_shape=(28,28,1)),\n",
    "                                          tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "                                          tf.keras.layers.Conv2D(64,(3,3),activation=tf.nn.relu),\n",
    "                                          tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "                                          tf.keras.layers.Flatten(),\n",
    "                                          tf.keras.layers.Dense(128,activation=tf.nn.relu),\n",
    "                                          tf.keras.layers.Dense(256,activation=tf.nn.relu),\n",
    "                                          tf.keras.layers.Dense(10,activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 259,338\n",
      "Trainable params: 259,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print Model summary:\n",
    "model_exp_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure the model for training:\n",
    "model_exp_5.compile(optimizer='adam',\n",
    "                   loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.4547 - acc: 0.8333\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.2930 - acc: 0.8922\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 75s 1ms/sample - loss: 0.2475 - acc: 0.9086\n",
      "Epoch 4/5\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9196\n",
      "[MESSAGE]::Reached 91% accuracy so stopped further training!\n",
      "60000/60000 [==============================] - 80s 1ms/sample - loss: 0.2155 - acc: 0.9196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22778684888>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model with Training data and specify number of epochs:\n",
    "model_exp_5.fit(train_images,train_labels,epochs=5,callbacks=[call_back])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 452us/sample - loss: 0.2610 - acc: 0.9037\n",
      "[TEST LOSS]:: 0.2610350433886051\n",
      "[TEST ACCURACY]:: 0.9037\n"
     ]
    }
   ],
   "source": [
    "#Let's evaluate model on Validation dataset and note test loss and test accuracy:\n",
    "test_loss,test_accuracy = model_exp_5.evaluate(test_images,test_labels)\n",
    "print(\"[TEST LOSS]::\",test_loss)\n",
    "print(\"[TEST ACCURACY]::\",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !----------------JUST EXPERIMENT---------------------!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.2'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 14s 226us/sample - loss: 0.6002 - acc: 0.8545\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 13s 219us/sample - loss: 0.3215 - acc: 0.9106\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 13s 221us/sample - loss: 0.2739 - acc: 0.9236\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 13s 221us/sample - loss: 0.2437 - acc: 0.9326\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 14s 225us/sample - loss: 0.2207 - acc: 0.9387\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 13s 222us/sample - loss: 0.2021 - acc: 0.9438\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 13s 219us/sample - loss: 0.1861 - acc: 0.9483\n",
      "Epoch 8/10\n",
      "59840/60000 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9519\n",
      "[MESSAGE]::Reached 95.19 Accuracy.So,the Model Training has stopped..!\n",
      "60000/60000 [==============================] - 13s 218us/sample - loss: 0.1728 - acc: 0.9519\n",
      "\n",
      "[TRAINING HISTORY]-->\n",
      ":: {'loss': [0.6002270574490229, 0.3214635694702466, 0.27391503539482753, 0.24371347932020823, 0.22074960346122582, 0.20209162058532237, 0.18605757680336635, 0.1728227297961712], 'acc': [0.8544667, 0.9105667, 0.92361665, 0.93255, 0.93873334, 0.9438, 0.9483, 0.9519]}\n",
      "\n",
      "[REQUIRED EPOCHS TO REACH DESIRED ACCURACY]-->\n",
      ":: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\n",
      "[ACCURACY THRESHOLD AT LAST EPOCH]-->\n",
      ":: 0.9519\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "def train_set():\n",
    "    class MyCallBacks(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self,epoch,logs={}):\n",
    "            if(logs.get('acc') > 0.95):\n",
    "                print('\\n[MESSAGE]::Reached %.2f Accuracy.So,the Model Training has stopped..!'%(logs.get('acc')*100))\n",
    "                self.model.stop_training=True\n",
    "    callbacks=MyCallBacks()\n",
    "    mnist_dataset=tf.keras.datasets.mnist \n",
    "    (x_train,y_train),(x_test,y_test)=mnist_dataset.load_data()\n",
    "    x_train=x_train/255.0\n",
    "    x_test=x_test/255.0\n",
    "    classifier=tf.keras.Sequential([\n",
    "                                    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "                                    tf.keras.layers.Dense(512,activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
    "                                    ])\n",
    "    classifier.compile(\n",
    "                        optimizer='sgd',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy']\n",
    "                       )    \n",
    "    history=classifier.fit(x_train,y_train,epochs=10,callbacks=[callbacks])\n",
    "    return history.history,history.epoch,history.history['acc'][-1]\n",
    "#Call the function::\n",
    "training_history,required_epochs,accuracy_threshold = train_set()\n",
    "print(\"\\n[TRAINING HISTORY]-->\\n::\",training_history)\n",
    "print(\"\\n[REQUIRED EPOCHS TO REACH DESIRED ACCURACY]-->\\n::\",required_epochs)\n",
    "print(\"\\n[ACCURACY THRESHOLD AT LAST EPOCH]-->\\n::\",accuracy_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ! END !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
